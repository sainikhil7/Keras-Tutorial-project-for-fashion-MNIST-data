{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Keras Tutorial Project(Team 15) : Harsha Sai Teja Gannamani, Venkata Sai Nikhil Meda, Yashwanth Bandala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import warnings\n",
    "from keras.optimizers import SGD\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Question 1<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1) Loading data\n",
    "Choose a data set that fits a regression or classification problem. Describe those data and show how to load those data so that they can be used in keras. Show how to create cross-validation folds, etc., so that you can test your algorithm properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have choosen Fashion_MNIST dataset which fits for the classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Importing the Fashion_MNIST Data Set</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion_MNIST Data set contains 70,000 grey scale images of 10 categories clothing as (28x28) pixel images. Here we use 60000 images as Training Data and rest 10000 as Testing Data. This Data Set is available in Keras. So, we can use it from the Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Exploring the Data Set<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting the images of the 10 different categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Different Categorical Variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>T-shirt/top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Trouser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Pullover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Dress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Coat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Sandal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Sneaker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Bag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Ankle boot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Different Categorical Variables\n",
       "0                     T-shirt/top\n",
       "1                         Trouser\n",
       "2                        Pullover\n",
       "3                           Dress\n",
       "4                            Coat\n",
       "5                          Sandal\n",
       "6                           Shirt\n",
       "7                         Sneaker\n",
       "8                             Bag\n",
       "9                      Ankle boot"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seri=pd.Series(y_train)\n",
    "k=[]\n",
    "for i in range(10):\n",
    "    k.append(seri[seri == i].index[0])\n",
    "Labels=['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "df=pd.DataFrame(Labels,columns=['Different Categorical Variables'])\n",
    "df\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "j=0\n",
    "for i in k:\n",
    "    plt.subplot(3,4, j+1)\n",
    "    plt.imshow(x_train[i]) \n",
    "    #plt.colorbar()\n",
    "    plt.grid(False)\n",
    "    plt.xlabel(Labels[j])\n",
    "    j=j+1\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Preprocessing the Data<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[1])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above figure we can see that the pixel values are ranging from 0 to 255, Inorder to pass them to the neural network we need to scalae them down to range 0 to 1, So we are dividing the training data and testing data with 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_train.astype('float32')\n",
    "X_test = x_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we should have 10 different classes, one for each digit, but we have only have an 1-dimensional array. so we need to Convert 1-dimensional class arrays to 10-dimensional class matrices before passing them to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing Training and testing labels\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Flatenning our data of(28x28) 2D array into 1D array of 784 pixels to send it through the model<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny = X_train.shape\n",
    "Xtrain = X_train.reshape((nsamples,nx*ny))\n",
    "nsamples1, nx1, ny1 = X_test.shape\n",
    "Xtest= X_test.reshape((nsamples1,nx1*ny1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Cross validation</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a technique in which we train our model using the subset of the data-set and then evaluate using the complementary subset of the data-set.There are Different types of cross validation in machine learning among them the popular ones are k-fold cross validation,leaveoneout cross validation and Holdout method cross validation\n",
    "<li> We are using k-fold cross validation in this project because of it's reduced bias and reduced computational time when compared to other cross validations.\n",
    "<li> In this k-fold cross validation we divide the entire data set into k-folds and then train our model using k-1 folds and then we test our model on the kth fold.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Capture1.jpeg\",width=500,height=100>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"Capture1.jpeg\",width=500,height=100>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K fold cross validation function function\n",
    "\"\"\"\n",
    "Here Xtrain is the flattened (1D)training data with out labels and y_train is the label data and folds are number of folds\n",
    "It retuns a list of accuracies and losses for each fold\n",
    "\"\"\"\n",
    "def Kfold(Xtrain,y_train,folds,opt):\n",
    "    accuracy=[]\n",
    "    accuracy1=[]\n",
    "    loss1=[]\n",
    "    frame=[]\n",
    "    ren=Xtrain\n",
    "    dfre=pd.DataFrame(ren)#converting our (1D) numpy array into data frame\n",
    "    dfre['op']=y_train # adding our labels into the data frame\n",
    "    df = dfre.sample(frac=1) \n",
    "    \"\"\"\n",
    "    rearranging the rows in our data frame such that when we split the dataframe in to k folds \n",
    "    we have all the class labels distributed equally among all folds\n",
    "    \n",
    "    \"\"\"\n",
    "    df_split = np.array_split(df, folds)#Splitting our data frame into k folds\n",
    "    for i in range(len(df_split)):\n",
    "        frame.append(df_split[i])#Adding all our splitted k fold data frames into a list\n",
    "    for i in range(folds):\n",
    "        fram=[]\n",
    "        fram=frame.copy()\n",
    "        fram.pop(i)\n",
    "        \"\"\"\n",
    "        here in fram list we have all the data frames appended.so for example when i value is 1 we are popping the \n",
    "        ith(i.e 1st) positioned data frame from the list and the concatenate all the other dataframes in the list. Here\n",
    "        the ith(ie 1st) positoned data frame is the (k)testing fold and the rest of the data frames contains (k-1)folds\n",
    "        training data.\n",
    "        \"\"\"\n",
    "        print(\"For Fold {0}\".format(i))\n",
    "        print('-------------------------------')\n",
    "        train = pd.concat(fram)\n",
    "        test = frame[i]\n",
    "        trainY=train.iloc[:,-1]# removing the training data labels from the training data(dataframe) and setting them as training labels\n",
    "        testY=test.iloc[:,-1]# removing the testing data labels from the training data(dataframe) and setting them as training labels\n",
    "        trainX=train.iloc[:, :-1]#removing labels from the training data\n",
    "        testX=test.iloc[:, :-1]# removing labels from the testing data\n",
    "        X_train = trainX.astype('float32')\n",
    "        X_test = testX.astype('float32')\n",
    "        \"\"\"\n",
    "        Here we are converting the labels of training and testing data set into a  binary matrix representation.\n",
    "        The classes axis is placed last. As the number of different class labels are 10 we are passing the value of 10\n",
    "        \"\"\"\n",
    "        Y_train = np_utils.to_categorical(trainY, 10)\n",
    "        Y_test = np_utils.to_categorical(testY, 10)\n",
    "        \"\"\"\n",
    "        Then we will pass our training and testing data. In model we will define dense layers of perceptrons and then fit\n",
    "        our training data to the model and the predict accuracy and loss on the testing data.\n",
    "        \"\"\"\n",
    "        accuracy,loss,pred=modeln(X_train,Y_train,X_test,Y_test,opt)\n",
    "        print('-------------------------------')\n",
    "        print(\"Accuracy for the Fold {0} is {1}\".format(i,accuracy))\n",
    "        print(\"Loss for the Fold {0} is {1}\".format(i,loss))\n",
    "        print('-------------------------------')\n",
    "        accuracy1.append(accuracy)\n",
    "        loss1.append(loss)\n",
    "    return accuracy1,loss1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Question 2<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Creating a keras model (a simple network)\n",
    "Develop a keras model that can perform the classification or regression for the data set you chose. I recommend using simple Dense layers so that it trains quickly. Describe this model and show an illustration (figure) of it in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Importing all the required libraries from the keras</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> model = sequential() </h5>\n",
    "Models in Keras are defined as a sequence of layers. The Sequential model is a linear stack of layers. Sequential specifies to keras that we are creating model sequentially and the output of each layer we add is input to the next layer we specify. It is limited in that it does not allow you to create models that share layers or have multiple inputs or outputs. the layers can be defined and passed to the Sequential as an array. The Sequential model API is great for developing deep learning models in most situations. In Keras, we assemble layers to build models. A model is (usually) a graph of layers. The most common type of model is a stack of layers that is Sequential model.\n",
    "\n",
    "\n",
    "<h5> model.add(Dense(250, input_shape=(784,), activation='relu')) </h5>\n",
    "For Dense layers, the first parameter is the output size of the layer. Keras automatically handles the connections between layers. model.add is used to add a layer to our neural network. We need to specify as an argument what type of layer we want. The Dense is used to specify the fully connected layer. For this layer we have the input of 784 pixels(1D array).This layer contains 250 units (perceptrons). We are using the RELU activation function in this layer   \n",
    "\n",
    "<h5> model.add(Dense(100, activation='relu')) </h5>\n",
    "For Dense layers, the first parameter is the output size of the layer. Keras automatically handles the connections between layers. model.add is used to add a layer to our neural network. We need to specify as an argument what type of layer we want. The Dense is used to specify the fully connected layer.  For this layer we have the 250 inputs from the 250 units of the previous layer.This layer contains 250 units (perceptrons). We are using the RELU activation function in this layer\n",
    "<h5> model.add(Dense(10, activation='softmax')) </h5>\n",
    "The final layer has an output size of 10, corresponding to the 10 classes of digits. In our dataset, output is of 10 values. The output layer uses softmax activation function.For this layer we have the 100 inputs from the 100 units of the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ourmodel():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(250, input_shape=(784,), activation='relu'))\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Activation Function<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a neuron doesnot have an Activation function then it's output is just the linear transformation of inputs and weights.Then the network is acts a linear regression model which can only predict the linear relationship between the variables. So we use the activation function to introduce the non-linearity in the network. Some of the popular Activation functions used for classification problems are\n",
    "<li>Logistic\n",
    "<li>tanh(hyperbolic Tangent)\n",
    "<li>Relu(Rectified Linear Unit)\n",
    "<li>Leaky Relu\n",
    "<li>Softmax    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Why Relu?</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu(Rectified Linear Unit) is the most commonly used function because of it's simplicity and less computational time. Relu should always be used in the hidden layers because,they can have very large outputs so they might be expected to be far more likely to explode than units that have bounded values. Relu is treated as the standard activation function in the CNN(Convolutional Neural Network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification problem,we can use Sigmoid functions(Logistic, tanh, Softmax). But at this time we prefer using Relu because, sigmoid functions which are non-linear functions may suffer from vanishing gradient problem.A problem with training networks with many layers is that the gradient diminishes dramatically as it is propagated backward through the network. The error may be so small by the time it reaches layers close to the input of the model that it may have very little effect. As such, this problem is referred to as the “vanishing gradients” problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Function of Relu:</b> y=max(0,x)\n",
    "<li> Here X is the input,If the value of x is positive then the function returns x.\n",
    "<li> If the input value is negative or equal to zero then the function returns 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=[-5,-4,-3,-2,-1,0,1,2,3,4,5]\n",
    "y=[]\n",
    "for i in x:\n",
    "    y.append(max(0,i))\n",
    "plt.plot(x, y, '-')\n",
    "plt.title('Relu Function')\n",
    "plt.xlabel('input')\n",
    "plt.ylabel('output')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>why to use the soft max activation function in the last neural network</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a classification problem, the output will be the probability distribution containing different classes probability values. If we are considering a binary classification problem then we can use the logistic activation function, since we are considering the Multi-class classification problem logistic activation doesnot work well so we use softmax for multiclass classification problems. For example if we consider one of the output value to be negative then when we we calculate the probability of a class we will get the negative value, which is not acceptable. But in softmax even if the output value of a class is negative it returns positive value as e^x will always return positive values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Function of softmax</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"softmax.jpg\",width=500,height=100>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"softmax.jpg\",width=500,height=100>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> model.summary() </h5>\n",
    "model.summary() prints a summary representation of the model. For layers with multiple outputs, multiple is displayed instead of each individual output shape due to size limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_238 (Dense)            (None, 250)               196250    \n",
      "_________________________________________________________________\n",
      "dense_239 (Dense)            (None, 100)               25100     \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 222,360\n",
      "Trainable params: 222,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=ourmodel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model summary states that we have 3 dense layers having 250 units in the first layer, 100 in the second layer and 10 units in the last layers.\n",
    "<br><b>Hidden layer1</b>\n",
    "<li> number of paramenters in the 1st layer are (250x785), here 785 are the input pixels +bias and 250 are the units present in the layer.\n",
    "<br><b>Hidden layer2</b>\n",
    "<li> number of paramenters in the 2nd layer are (100x251), here 251 are the number of outputs from the previous layer (i.e 250) + bias and 100 are the number of units in the layer.\n",
    "<br><b>Output layer</b>\n",
    "<li> number of paramenters in the output layer are (10x101), here 101 are the number of outputs from the previous layer (i.e 100) + bias and 10 are the number of units in the layer      \n",
    "Total number of parameters in our neural network is 222,360 among them all are trainable parameters.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Our Model Architecture</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"model.jpeg\",width=500,height=100>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"model.jpeg\",width=500,height=100>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"model flow.jpeg\",width=500,height=100>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"model flow.jpeg\",width=500,height=100>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Model architecture contains 2 hidden layers and an output layer:\n",
    "<li> 1st hidden layer has 250 units(perceptrons) and uses RELU activation function\n",
    "<li> 2nd hidden layer has 100 units(perceptrons) and uses RELU activation function\n",
    "<li> output layer has 10 units(perceptrons) and uses Softmax activation function       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Question 3</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Compiling the model\n",
    "<li>Show how to compile the keras model, choosing the loss function, optimizer, etc. Describe the choices you made and why. Describe what other choices could be made here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ourmodel()\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model.compile \n",
    "<li>we used loss function of categorical_crossentropy\n",
    "<li>we used optimizer as Adam(Adaptive Moment Estimation)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHere score[1] is the accuracy of the testing data and score[0] is the loss of the testing data\\npred contains the list of predicted outputs for the testing data\\n'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Function modeln takes X_train,X_test(preprocessed testing and training data) and Y_Train,Y_test(preprocessed testing and \n",
    "training data mentioned in the 1st question)\n",
    "\n",
    "This function is used in the KFOLD function defined in the 1st question\n",
    "\"\"\"\n",
    "def modeln(X_train,Y_train,X_test,Y_test,opt):\n",
    "    model = ourmodel()#model is defined in the second question\n",
    "    model.compile(loss=\"categorical_crossentropy\",optimizer=opt,metrics=['accuracy'])#compliling the model\n",
    "    model.fit(X_train, Y_train, batch_size=128,nb_epoch=10)#fitting the model\n",
    "    score = model.evaluate(X_test, Y_test)#evaluating the model\n",
    "    score1 = model.predict(Xtest)\n",
    "    pred=[]\n",
    "    for i in range(len(score1)):\n",
    "        pred.append(np.argmax(score1[i]))\n",
    "    return score[1],score[0],pred\n",
    "\"\"\"\n",
    "Here score[1] is the accuracy of the testing data and score[0] is the loss of the testing data\n",
    "pred contains the list of predicted outputs for the testing data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 5 fold cross validation using ADAM as optimizer</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Fold 0\n",
      "-------------------------------\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 13s 263us/step - loss: 0.5442 - accuracy: 0.8102\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 6s 120us/step - loss: 0.3883 - accuracy: 0.8613\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.3459 - accuracy: 0.8744\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.3141 - accuracy: 0.8839\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 6s 128us/step - loss: 0.2983 - accuracy: 0.88972s - loss: - ETA: 2s - l\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.2829 - accuracy: 0.8947\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.2725 - accuracy: 0.8989\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.2574 - accuracy: 0.9025\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 7s 140us/step - loss: 0.2460 - accuracy: 0.9087\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 6s 126us/step - loss: 0.2377 - accuracy: 0.9110\n",
      "12000/12000 [==============================] - 3s 224us/step\n",
      "-------------------------------\n",
      "Accuracy for the Fold 0 is 0.887333333492279\n",
      "Loss for the Fold 0 is 0.3257365407049656\n",
      "-------------------------------\n",
      "For Fold 1\n",
      "-------------------------------\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 7s 156us/step - loss: 0.5293 - accuracy: 0.8138\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.3841 - accuracy: 0.8620\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.3405 - accuracy: 0.8758\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.3139 - accuracy: 0.8841\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 6s 126us/step - loss: 0.2920 - accuracy: 0.8931\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 7s 136us/step - loss: 0.2768 - accuracy: 0.8979\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 6s 135us/step - loss: 0.2624 - accuracy: 0.9020\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.2514 - accuracy: 0.9063\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 7s 137us/step - loss: 0.2408 - accuracy: 0.9101\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.2326 - accuracy: 0.9112\n",
      "12000/12000 [==============================] - 3s 221us/step\n",
      "-------------------------------\n",
      "Accuracy for the Fold 1 is 0.8797500133514404\n",
      "Loss for the Fold 1 is 0.3392549906373024\n",
      "-------------------------------\n",
      "For Fold 2\n",
      "-------------------------------\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 7s 144us/step - loss: 0.5389 - accuracy: 0.81240s - loss: 0.5457 - ac\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 7s 136us/step - loss: 0.3924 - accuracy: 0.8590\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.3458 - accuracy: 0.8742\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 7s 137us/step - loss: 0.3159 - accuracy: 0.8842\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 7s 136us/step - loss: 0.2983 - accuracy: 0.8893\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.2845 - accuracy: 0.89380s - loss: 0.2834 - accura\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 7s 139us/step - loss: 0.2710 - accuracy: 0.89920s - loss: 0.2706 - ac\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.2588 - accuracy: 0.9024\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.2453 - accuracy: 0.9079\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 7s 140us/step - loss: 0.2391 - accuracy: 0.9092\n",
      "12000/12000 [==============================] - 3s 220us/step\n",
      "-------------------------------\n",
      "Accuracy for the Fold 2 is 0.8974999785423279\n",
      "Loss for the Fold 2 is 0.2843681688408057\n",
      "-------------------------------\n",
      "For Fold 3\n",
      "-------------------------------\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 8s 160us/step - loss: 0.5399 - accuracy: 0.8133\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 7s 136us/step - loss: 0.3867 - accuracy: 0.8615\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 7s 137us/step - loss: 0.3414 - accuracy: 0.87690s - loss: 0.3\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 6s 134us/step - loss: 0.3152 - accuracy: 0.8855\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 7s 143us/step - loss: 0.2939 - accuracy: 0.8920\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 6s 133us/step - loss: 0.2771 - accuracy: 0.8969\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 7s 138us/step - loss: 0.2661 - accuracy: 0.90160s - loss: 0.2662 - \n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - ETA: 0s - loss: 0.2583 - accuracy: 0.90 - 6s 131us/step - loss: 0.2582 - accuracy: 0.9035\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.2434 - accuracy: 0.9098\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 7s 147us/step - loss: 0.2323 - accuracy: 0.91370s - loss: 0.2325 - accuracy: \n",
      "12000/12000 [==============================] - 3s 220us/step\n",
      "-------------------------------\n",
      "Accuracy for the Fold 3 is 0.8896666765213013\n",
      "Loss for the Fold 3 is 0.3134968989392122\n",
      "-------------------------------\n",
      "For Fold 4\n",
      "-------------------------------\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 7s 153us/step - loss: 0.5420 - accuracy: 0.8097\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 7s 135us/step - loss: 0.3881 - accuracy: 0.8621\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 7s 142us/step - loss: 0.3478 - accuracy: 0.8742\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 6s 135us/step - loss: 0.3170 - accuracy: 0.8830\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 7s 142us/step - loss: 0.3002 - accuracy: 0.88950s - loss: 0.3005 - ac\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 7s 136us/step - loss: 0.2841 - accuracy: 0.8953\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 7s 148us/step - loss: 0.2719 - accuracy: 0.9003\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 7s 139us/step - loss: 0.2574 - accuracy: 0.9035\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 6s 134us/step - loss: 0.2466 - accuracy: 0.9069\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 7s 147us/step - loss: 0.2380 - accuracy: 0.9116\n",
      "12000/12000 [==============================] - 3s 273us/step\n",
      "-------------------------------\n",
      "Accuracy for the Fold 4 is 0.887666642665863\n",
      "Loss for the Fold 4 is 0.31260388469696043\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHere Xtrain is the preprocessed training data\\ny_train is not preprocessed data as we will be performing preprocessing for the y_train data before evaluating the model in the\\nkfold function in the first function.\\n5 is number of folds for cross validation\\n\"ADAM\"(Adaptive Moment estimation) is the optimizer used for the model\\n'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(l,p)=Kfold(Xtrain,y_train,5,\"ADAM\")\n",
    "\"\"\"\n",
    "Here Xtrain is the preprocessed training data\n",
    "y_train is not preprocessed data as we will be performing preprocessing for the y_train data before evaluating the model in the\n",
    "kfold function in the first function.\n",
    "5 is number of folds for cross validation\n",
    "\"ADAM\"(Adaptive Moment estimation) is the optimizer used for the model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR ADAM Optimizer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folds</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Fold 0</td>\n",
       "      <td>0.887333</td>\n",
       "      <td>0.325737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Fold 1</td>\n",
       "      <td>0.879750</td>\n",
       "      <td>0.339255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Fold 2</td>\n",
       "      <td>0.897500</td>\n",
       "      <td>0.284368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Fold 3</td>\n",
       "      <td>0.889667</td>\n",
       "      <td>0.313497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Fold 4</td>\n",
       "      <td>0.887667</td>\n",
       "      <td>0.312604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Avg</td>\n",
       "      <td>0.888383</td>\n",
       "      <td>0.315092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Folds  Accuracy      loss\n",
       "0  Fold 0  0.887333  0.325737\n",
       "1  Fold 1  0.879750  0.339255\n",
       "2  Fold 2  0.897500  0.284368\n",
       "3  Fold 3  0.889667  0.313497\n",
       "4  Fold 4  0.887667  0.312604\n",
       "5     Avg  0.888383  0.315092"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis1=['Fold 0','Fold 1','Fold 2','Fold 3','Fold 4','Avg']\n",
    "l.append(sum(l)/5)\n",
    "p.append(sum(p)/5)\n",
    "r=list(zip(lis1,l,p))\n",
    "print('FOR ADAM Optimizer')\n",
    "data=pd.DataFrame(r,columns=['Folds','Accuracy','loss'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 5 fold cross validation using SGD as optimizer</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Fold 0\n",
      "-------------------------------\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 1.0816 - accuracy: 0.6639\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.6730 - accuracy: 0.7801\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.5845 - accuracy: 0.8067\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.5373 - accuracy: 0.8189\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 5s 104us/step - loss: 0.5069 - accuracy: 0.82750s - loss: 0.5080 - accuracy: 0.\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 5s 99us/step - loss: 0.4858 - accuracy: 0.8328\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.4690 - accuracy: 0.8370\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.4563 - accuracy: 0.8427\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 4s 92us/step - loss: 0.4463 - accuracy: 0.8449 0s\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.4364 - accuracy: 0.84900s - loss: 0.4370 \n",
      "12000/12000 [==============================] - 3s 221us/step\n",
      "-------------------------------\n",
      "Accuracy for the Fold 0 is 0.8512499928474426\n",
      "Loss for the Fold 0 is 0.43843886550267536\n",
      "-------------------------------\n",
      "For Fold 1\n",
      "-------------------------------\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 1.0954 - accuracy: 0.6684\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.6708 - accuracy: 0.7799\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.5827 - accuracy: 0.8065\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 5s 104us/step - loss: 0.5371 - accuracy: 0.8188\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 6s 116us/step - loss: 0.5080 - accuracy: 0.8266\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 5s 106us/step - loss: 0.4871 - accuracy: 0.8332\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 5s 105us/step - loss: 0.4718 - accuracy: 0.8376\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.4593 - accuracy: 0.8416\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.4495 - accuracy: 0.8446\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 5s 106us/step - loss: 0.4389 - accuracy: 0.8485\n",
      "12000/12000 [==============================] - 3s 226us/step\n",
      "-------------------------------\n",
      "Accuracy for the Fold 1 is 0.8397499918937683\n",
      "Loss for the Fold 1 is 0.4528316346009572\n",
      "-------------------------------\n",
      "For Fold 2\n",
      "-------------------------------\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 6s 119us/step - loss: 1.1169 - accuracy: 0.6610\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 5s 98us/step - loss: 0.6844 - accuracy: 0.7791 0s - loss: 0.6\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.5869 - accuracy: 0.8089\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.5379 - accuracy: 0.8207\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 5s 106us/step - loss: 0.5066 - accuracy: 0.8292\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 5s 98us/step - loss: 0.4853 - accuracy: 0.8341\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.4696 - accuracy: 0.8396\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.4576 - accuracy: 0.8425\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 5s 99us/step - loss: 0.4463 - accuracy: 0.8466\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 5s 98us/step - loss: 0.4382 - accuracy: 0.8484\n",
      "12000/12000 [==============================] - 3s 239us/step\n",
      "-------------------------------\n",
      "Accuracy for the Fold 2 is 0.8519166707992554\n",
      "Loss for the Fold 2 is 0.428960071404775\n",
      "-------------------------------\n",
      "For Fold 3\n",
      "-------------------------------\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 1.1578 - accuracy: 0.6357\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.6816 - accuracy: 0.7748\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 5s 99us/step - loss: 0.5848 - accuracy: 0.8050\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.5370 - accuracy: 0.8197\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.5078 - accuracy: 0.8273\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.4873 - accuracy: 0.8327\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.4717 - accuracy: 0.8367\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 5s 99us/step - loss: 0.4591 - accuracy: 0.8409\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 5s 98us/step - loss: 0.4492 - accuracy: 0.8441\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.4400 - accuracy: 0.8466 0s - loss: 0.4405 - \n",
      "12000/12000 [==============================] - 3s 227us/step\n",
      "-------------------------------\n",
      "Accuracy for the Fold 3 is 0.847000002861023\n",
      "Loss for the Fold 3 is 0.443106813232104\n",
      "-------------------------------\n",
      "For Fold 4\n",
      "-------------------------------\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 5s 109us/step - loss: 1.1381 - accuracy: 0.6517\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 5s 94us/step - loss: 0.6829 - accuracy: 0.7745\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 5s 107us/step - loss: 0.5886 - accuracy: 0.80520s - loss: 0.5886 - accuracy: 0.80\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 6s 126us/step - loss: 0.5404 - accuracy: 0.8183\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.5096 - accuracy: 0.8267\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.4893 - accuracy: 0.8316\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 5s 94us/step - loss: 0.4728 - accuracy: 0.8375\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 5s 99us/step - loss: 0.4604 - accuracy: 0.8428\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.4505 - accuracy: 0.84490s - loss: 0.4497 - accuracy: \n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.4421 - accuracy: 0.8476\n",
      "12000/12000 [==============================] - 3s 218us/step\n",
      "-------------------------------\n",
      "Accuracy for the Fold 4 is 0.8460833430290222\n",
      "Loss for the Fold 4 is 0.4436290881633759\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHere Xtrain is the preprocessed training data\\ny_train is not preprocessed data as we will be performing preprocessing for the y_train data before evaluating the model in the\\nkfold function in the first function.\\n5 is number of folds for cross validation\\n\"SGD\" (Stochastic Gradient Descent) is the optimizer used for the model\\n'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(l3,p3)=Kfold(Xtrain,y_train,5,\"SGD\")\n",
    "\"\"\"\n",
    "Here Xtrain is the preprocessed training data\n",
    "y_train is not preprocessed data as we will be performing preprocessing for the y_train data before evaluating the model in the\n",
    "kfold function in the first function.\n",
    "5 is number of folds for cross validation\n",
    "\"SGD\" (Stochastic Gradient Descent) is the optimizer used for the model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR SGD Optimizer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folds</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Fold 0</td>\n",
       "      <td>0.851250</td>\n",
       "      <td>0.438439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Fold 1</td>\n",
       "      <td>0.839750</td>\n",
       "      <td>0.452832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Fold 2</td>\n",
       "      <td>0.851917</td>\n",
       "      <td>0.428960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Fold 3</td>\n",
       "      <td>0.847000</td>\n",
       "      <td>0.443107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Fold 4</td>\n",
       "      <td>0.846083</td>\n",
       "      <td>0.443629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Avg</td>\n",
       "      <td>0.847200</td>\n",
       "      <td>0.441393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Folds  Accuracy      loss\n",
       "0  Fold 0  0.851250  0.438439\n",
       "1  Fold 1  0.839750  0.452832\n",
       "2  Fold 2  0.851917  0.428960\n",
       "3  Fold 3  0.847000  0.443107\n",
       "4  Fold 4  0.846083  0.443629\n",
       "5     Avg  0.847200  0.441393"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis2=['Fold 0','Fold 1','Fold 2','Fold 3','Fold 4','Avg']\n",
    "l3.append(sum(l3)/5)\n",
    "p3.append(sum(p3)/5)\n",
    "r1=list(zip(lis2,l3,p3))\n",
    "print('FOR SGD Optimizer')\n",
    "data=pd.DataFrame(r1,columns=['Folds','Accuracy','loss'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Loss function — It is used to measure how accurate our model is during the training.It is used to minimize the weights toto \"steer\" the model in the right direction.\n",
    "<li>Optimizer —It is used to update our weights based on the data it sees and its loss function.\n",
    "<li>Metrics —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Why we choose ADAM Optimizer</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used Adam(Adaptive Moment Estimation) optimizer for this model, because Adam optimizer works well in practice and outperforms other Adaptive techniques. Adam can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum. ADAM uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum. \n",
    "\n",
    "we have ran 5 fold cross validation on training data set using optimizers Adam and SGD. <b>We got more accuracy for ADAM(88%) when compared tp SGD(85%).So Adam is the best optimizer for this Data Set.</b> SGD is a variant of gradient descent. Instead of performing computations on the whole dataset which is redundant and inefficient SGD only computes on a small subset or random selection of data examples. But Adam is an algorithm for gradient-based optimization of stochastic objective functions. Adam is an adaptive learning rate method, which means, it computes individual learning rates for different parameters.For stochastic gradient descent batch size should be 1, so it will take a lot of time to compute. So we opt for ADAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>What other optimizers can we choose</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can choose other optimizers like SGD and RMSprop for the multi-class classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Loss Function</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can dive the loss functions into 3 types based on the type of problems:\n",
    "<li>1) Regression Loss Function \n",
    "<li>2) Binary Classification Loss Function \n",
    "<li>3) Multi-Class Classification Loss Function\n",
    "\n",
    "<br>In regression loss function: \n",
    "<li>i) Mean Squared Error Loss \n",
    "<li>ii) Mean Squared Logarithmic Error Loss \n",
    "<li>iii) Mean Absolute Error Loss\n",
    "\n",
    "<br>For Binary Classification we use Loss Functions like: \n",
    "<li>i) Binary Cross-Entropy \n",
    "<li>ii) Hinge Loss \n",
    "<li>iii) Squared Hinge Loss\n",
    "\n",
    "<br>For Multi-Class Classification we use Loss Functions like: \n",
    "<li>i) Multi-Class Cross-Entropy Loss \n",
    "<li>ii) Sparse Multiclass Cross-Entropy Loss \n",
    "<li>iii) Kullback Leibler Divergence Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Why we choose cross-entropy loss function</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our dataset(MNIST fashion) fits for a Multi-class classification problem. So we considered Multi-Class Cross-Entropy Loss. \n",
    "\n",
    "Cross-entropy is the default loss function used in multi-class classification problems. The target values for this loss function are in the set {0, 1, 3, …, n}, where each class is assigned to a unique integer value. The loss function is to be evaluated first and then only changed if we have a good reason. Cross-entropy calculates the score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0. Cross-entropy can be specified as the loss function in Keras by specifying ‘categorical_crossentropy‘ when compiling the model.Cross-entropy loss increases as the predicted probability diverges from the actual label and since it is default loss function we used the cross-entopy loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> What other loss functions can we use </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use other loss functions like Sparse Multiclass Cross-Entropy Loss and Kullback Leibler Divergence Loss for the multi-classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Question 4</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers like stochastic gradient descent and Adaptive Moment Estimation uses different hyperparameters like epochs and Batch Size. ADAM and stochastic gradient descent are iterative learning algorithms\n",
    "<li>The batch size is a hyperparameter of gradient descent that controls the number of training samples to work through before the model’s internal parameters are updated.\n",
    "<li>The number of epochs is a hyperparameter of gradient descent that controls the number of complete passes through the training dataset.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>How to select the number of epochs<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot fix how to select the number of epochs to be used by seeing the dataset. we can fix the number of epochs to be used based on the validation and the training error. For instance, if the validation error starts increasing that might be a indication of overfitting. You should set the number of epochs as high as possible and terminate training based on the error rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>How to select the batch size<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size impacts significant learning. The concept is, if our batch size is large, this will provide a enough estimate of gradient of the full dataset. By taking samples from our dataset, we can estimate the gradient while reducing computational cost. We will get less accuracy estimate if we go lower. The noisy gradients actually helps to escape local minima. When noisy gradient is too low, our network weights can just jump around if your data is noisy and it converges very slowly results in negatively impacts total computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.5147 - accuracy: 0.8202\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.3731 - accuracy: 0.8653\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.3292 - accuracy: 0.8800\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.3086 - accuracy: 0.8863\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.2872 - accuracy: 0.8944\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.2717 - accuracy: 0.8984\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.2607 - accuracy: 0.9031\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.2475 - accuracy: 0.9070\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.2421 - accuracy: 0.9083\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.2310 - accuracy: 0.9137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1dc3c85b848>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain, Y_train, batch_size=128, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model.fit \n",
    "<li>X_train is the training data\n",
    "<li>Y_train are the training labels\n",
    "<li>we took batch size value of 128,such that we will take 128 samples of training data to work through before the model’s internal parameters are updated. \n",
    "<li>We took the number of epochs as 10,such that we iterate through the training data 10 times to get the performance of our model.    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>performing 5 Fold cross-validation on our training data</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Fold 0\n",
      "-------------------------------\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 7s 142us/step - loss: 0.5353 - accuracy: 0.8111\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.3793 - accuracy: 0.8629\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.3420 - accuracy: 0.8749\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.3108 - accuracy: 0.8869\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.2930 - accuracy: 0.89130s - loss: 0.2923 \n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.2804 - accuracy: 0.8964\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 7s 139us/step - loss: 0.2674 - accuracy: 0.90001s - loss: 0.2664 - accura - ETA: 0s\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.2557 - accuracy: 0.9050\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.2437 - accuracy: 0.9080\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.2335 - accuracy: 0.9127\n",
      "12000/12000 [==============================] - 3s 226us/step\n",
      "-------------------------------\n",
      "Accuracy for the Fold 0 is 0.8884166479110718\n",
      "Loss for the Fold 0 is 0.3161510165135066\n",
      "-------------------------------\n",
      "For Fold 1\n",
      "-------------------------------\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 9s 179us/step - loss: 0.5389 - accuracy: 0.8119\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 6s 126us/step - loss: 0.3848 - accuracy: 0.8627\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 7s 137us/step - loss: 0.3456 - accuracy: 0.8741\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 7s 142us/step - loss: 0.3183 - accuracy: 0.8846\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.2969 - accuracy: 0.8910\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.2812 - accuracy: 0.89700s\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.2677 - accuracy: 0.9006\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 6s 133us/step - loss: 0.2529 - accuracy: 0.9064\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.2457 - accuracy: 0.9085\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 6s 127us/step - loss: 0.2346 - accuracy: 0.91250s - loss: 0.2346 - accuracy: 0.91\n",
      "12000/12000 [==============================] - 3s 218us/step\n",
      "-------------------------------\n",
      "Accuracy for the Fold 1 is 0.8825833201408386\n",
      "Loss for the Fold 1 is 0.3193208811879158\n",
      "-------------------------------\n",
      "For Fold 2\n",
      "-------------------------------\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 7s 150us/step - loss: 0.5430 - accuracy: 0.80990s - los - ETA: 0s - loss: 0.5435 - accuracy: 0.80\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 7s 139us/step - loss: 0.3858 - accuracy: 0.86220s - loss: 0.3877 \n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.3464 - accuracy: 0.8735\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.3129 - accuracy: 0.8850\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.2985 - accuracy: 0.8889\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 7s 144us/step - loss: 0.2809 - accuracy: 0.8955\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 6s 133us/step - loss: 0.2672 - accuracy: 0.9011\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.2564 - accuracy: 0.9036\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.2458 - accuracy: 0.9074\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 6s 127us/step - loss: 0.2323 - accuracy: 0.9128\n",
      "12000/12000 [==============================] - 3s 247us/step\n",
      "-------------------------------\n",
      "Accuracy for the Fold 2 is 0.8886666893959045\n",
      "Loss for the Fold 2 is 0.3186488593816757\n",
      "-------------------------------\n",
      "For Fold 3\n",
      "-------------------------------\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 7s 138us/step - loss: 0.5421 - accuracy: 0.8098\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.3854 - accuracy: 0.8616\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 6s 126us/step - loss: 0.3452 - accuracy: 0.8732\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.3187 - accuracy: 0.8834\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.3023 - accuracy: 0.8884\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 6s 126us/step - loss: 0.2840 - accuracy: 0.89430s - los\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.2696 - accuracy: 0.8990\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.2573 - accuracy: 0.9028\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 7s 143us/step - loss: 0.2471 - accuracy: 0.9076\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 6s 126us/step - loss: 0.2407 - accuracy: 0.9099\n",
      "12000/12000 [==============================] - 3s 222us/step\n",
      "-------------------------------\n",
      "Accuracy for the Fold 3 is 0.8930833339691162\n",
      "Loss for the Fold 3 is 0.300964307030042\n",
      "-------------------------------\n",
      "For Fold 4\n",
      "-------------------------------\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 7s 147us/step - loss: 0.5406 - accuracy: 0.8095\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 7s 138us/step - loss: 0.3817 - accuracy: 0.8621\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.3361 - accuracy: 0.8788\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.3123 - accuracy: 0.8851\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 6s 126us/step - loss: 0.2908 - accuracy: 0.8934\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 6s 120us/step - loss: 0.2740 - accuracy: 0.8991\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 6s 120us/step - loss: 0.2608 - accuracy: 0.9044\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 6s 126us/step - loss: 0.2478 - accuracy: 0.90750s - loss: 0.2481 - \n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 6s 120us/step - loss: 0.2426 - accuracy: 0.9083\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.2309 - accuracy: 0.9136\n",
      "12000/12000 [==============================] - 3s 215us/step\n",
      "-------------------------------\n",
      "Accuracy for the Fold 4 is 0.8916666507720947\n",
      "Loss for the Fold 4 is 0.3021725685497125\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHere Xtrain is the preprocessed training data\\ny_train is not preprocessed data as we will be performing preprocessing for the y_train data before evaluating the model in the\\nkfold function in the first function.\\n5 is number of folds\\nADAM is the optimizer used in the model\\n'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(l4,p4)=Kfold(Xtrain,y_train,5,\"ADAM\")# K Fold cross function is defined in the 1st question\n",
    "\"\"\"\n",
    "Here Xtrain is the preprocessed training data\n",
    "y_train is not preprocessed data as we will be performing preprocessing for the y_train data before evaluating the model in the\n",
    "kfold function in the first function.\n",
    "5 is number of folds\n",
    "ADAM is the optimizer used in the model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Accuracy and loss for each fold </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folds</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Fold 0</td>\n",
       "      <td>0.888417</td>\n",
       "      <td>0.316151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Fold 1</td>\n",
       "      <td>0.882583</td>\n",
       "      <td>0.319321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Fold 2</td>\n",
       "      <td>0.888667</td>\n",
       "      <td>0.318649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Fold 3</td>\n",
       "      <td>0.893083</td>\n",
       "      <td>0.300964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Fold 4</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.302173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Avg</td>\n",
       "      <td>0.888883</td>\n",
       "      <td>0.311452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Folds  Accuracy      loss\n",
       "0  Fold 0  0.888417  0.316151\n",
       "1  Fold 1  0.882583  0.319321\n",
       "2  Fold 2  0.888667  0.318649\n",
       "3  Fold 3  0.893083  0.300964\n",
       "4  Fold 4  0.891667  0.302173\n",
       "5     Avg  0.888883  0.311452"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis4=['Fold 0','Fold 1','Fold 2','Fold 3','Fold 4','Avg']\n",
    "l4.append(sum(l4)/5)\n",
    "p4.append(sum(p4)/5)\n",
    "r4=list(zip(lis4,l4,p4))\n",
    "data=pd.DataFrame(r4,columns=['Folds','Accuracy','loss'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing 5 fold cross validation on the Training Data we have obtained Training accuracy of 88%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Question 5</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Evaluation on the testing data</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>After performing model tuning for different optimizers using 5 fold cross validation. we have chosen ADAM optimizer is best suited for this dataset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.5138 - accuracy: 0.8194\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.3696 - accuracy: 0.8671\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.3288 - accuracy: 0.8799\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.3082 - accuracy: 0.8873\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.2868 - accuracy: 0.8946\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.2741 - accuracy: 0.8984\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.2618 - accuracy: 0.9032\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2486 - accuracy: 0.9070\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.2392 - accuracy: 0.9103\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.2316 - accuracy: 0.9133\n",
      "10000/10000 [==============================] - 2s 225us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFunction modeln takes Xtrain,Xtest(preprocessed testing and training data) and Y_Train,Y_test(preprocessed testing and \\ntraining data mentioned in the 1st question)\\n\\nThis function is used in the KFOLD function defined in the 1st question\\n\\nHere l1 is the accuracy on the testing data\\nli is the loss in testing data\\npred1 is the predicted labels of the testing data\\n\"ADAM\" optimizer used in our model\\n\\n'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(l1,a1,pred1)=modeln(Xtrain,Y_train,Xtest,Y_test,\"ADAM\")\n",
    "\"\"\"\n",
    "Function modeln takes Xtrain,Xtest(preprocessed testing and training data) and Y_Train,Y_test(preprocessed testing and \n",
    "training data mentioned in the 1st question)\n",
    "\n",
    "This function is used in the KFOLD function defined in the 1st question\n",
    "\n",
    "Here l1 is the accuracy on the testing data\n",
    "li is the loss in testing data\n",
    "pred1 is the predicted labels of the testing data\n",
    "\"ADAM\" optimizer used in our model\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Accuracy </h5>\n",
    "An accuracy metric is used to measure the algorithm’s performance in an interpretable way. Accuracy of a model is usually determined after the model parameters and is calculated in the form of a percentage. It is the measure of how accurate your model's prediction is compared to the true data.\n",
    "<li> Ex: If we have 100 test samples and the model only classifies 90 of them correctly then the model's accuracy is 90%. </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data Accuracy is 0.8805000185966492\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Data Accuracy is {0}\".format(l1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Classification Report</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>F1 Scores</h5>\n",
    "It is the harmonic mean of precision and recall. This takes the contribution of both, so higher the F1 score, the better. See that due to the product in the numerator if one goes low, the final F1 score goes down significantly. So a model does well in F1 score if the positive predicted are actually positives (precision) and doesn't miss out on positives and predicts them negative (recall).\n",
    "\n",
    "<h5> Precision </h5>\n",
    "Percentage of positive instances out of the total predicted positive instances. Here denominator is the model prediction done as positive from the whole given dataset. Take it as to find out ‘how much the model is right when it says it is right’.\n",
    "Precision = True Positive/(True Positive + False Positive).\n",
    "\n",
    "<h5> Recall </h5>\n",
    "Percentage of positive instances out of the total actual positive instances. Therefore denominator (TP + FN) here is the actual number of positive instances present in the dataset. Take it as to find out ‘how much extra right ones, the model missed when it showed the right ones’.\n",
    "Recall = True Positive/(True Positive + False Negative).\n",
    "\n",
    "<h5> Support </h5>\n",
    "The scores corresponding to every class will tell you the accuracy of the classifier in classifying the data points in that particular class compared to all other classes. The support is the number of samples of the true response that lie in that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83      1000\n",
      "           1       0.97      0.98      0.97      1000\n",
      "           2       0.73      0.87      0.79      1000\n",
      "           3       0.90      0.89      0.89      1000\n",
      "           4       0.84      0.73      0.78      1000\n",
      "           5       0.95      0.97      0.96      1000\n",
      "           6       0.73      0.66      0.69      1000\n",
      "           7       0.96      0.93      0.94      1000\n",
      "           8       0.99      0.95      0.97      1000\n",
      "           9       0.95      0.96      0.95      1000\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>AUC SCORE</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC score is used in classification analysis in order to determine which of the used models predicts the classes best.The AUC can be computed by adjusting the values in the matrix so that cells where the positive case outranks the negative case receive a 1 , cells where the negative case has higher rank receive a 0 , and cells with ties get 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score for the testing data is 0.9336111111111112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, auc\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "#roc_auc_score(y_test,pred1,average='weighted')\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_test)\n",
    "y_test1 = lb.transform(y_test)\n",
    "y_pred1 = lb.transform(pred1)\n",
    "print(\"AUC Score for the testing data is {0}\".format(roc_auc_score(y_test1, y_pred1, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Confusion Matrix</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[862,   2,  18,  16,   1,   0, 159,   0,   7,   0],\n",
       "       [  8, 979,   1,  19,   0,   0,   3,   0,   0,   0],\n",
       "       [ 28,   1, 867,  18, 166,   0,  99,   0,   8,   0],\n",
       "       [ 25,  12,   8, 892,  26,   0,  29,   0,   2,   0],\n",
       "       [  5,   4,  44,  34, 729,   0,  48,   0,   8,   0],\n",
       "       [  2,   0,   0,   0,   0, 975,   0,  31,   4,   9],\n",
       "       [ 66,   2,  62,  18,  78,   0, 656,   0,  14,   1],\n",
       "       [  0,   0,   0,   0,   0,   8,   0, 934,   3,  33],\n",
       "       [  4,   0,   0,   3,   0,   0,   6,   0, 954,   0],\n",
       "       [  0,   0,   0,   0,   0,  17,   0,  35,   0, 957]], dtype=int64)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X-axis Predicted vs Y-axis Actual Values\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(pred1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can say from the confusion matrix that label 0 is fasely predicted as label 6 for about 161 times in our testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestErrorDigits(pred,y_test):#This Function Will return a data frame containing testing error for each and every digit\n",
    "    rray=[0,0,0,0,0,0,0,0,0,0]\n",
    "    for i in range(len(y_test)):\n",
    "        if(pred[i]!=y_test[i]):#checking whether our predicted output and given label are same or not\n",
    "            rray[y_test[i]]=rray[y_test[i]]+1\n",
    "    err=[]\n",
    "    accu=[]\n",
    "    (unique, counts) = np.unique(y_test, return_counts=True)#This will give us the output how many times digits are present in the ytest \n",
    "    #print(len(rray))\n",
    "    for i in range(len(rray)):\n",
    "        err.append((rray[i]/counts[i])*100)\n",
    "        accu.append(100-((rray[i]/counts[i])*100))\n",
    "    d={'Label':[0,1,2,3,4,5,6,7,8,9],'Testing Error%':err,'Accuracy%':accu}\n",
    "    df=pd.DataFrame(d)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table for testing errors of each and every digit in the fashion_mnist dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Testing Error%</th>\n",
       "      <th>Accuracy%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>86.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>97.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13.3</td>\n",
       "      <td>86.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.8</td>\n",
       "      <td>89.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>27.1</td>\n",
       "      <td>72.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>97.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>34.4</td>\n",
       "      <td>65.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6.6</td>\n",
       "      <td>93.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>95.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>95.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  Testing Error%  Accuracy%\n",
       "0      0            13.8       86.2\n",
       "1      1             2.1       97.9\n",
       "2      2            13.3       86.7\n",
       "3      3            10.8       89.2\n",
       "4      4            27.1       72.9\n",
       "5      5             2.5       97.5\n",
       "6      6            34.4       65.6\n",
       "7      7             6.6       93.4\n",
       "8      8             4.6       95.4\n",
       "9      9             4.3       95.7"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table=TestErrorDigits(pred1,y_test)\n",
    "print(\"Table for testing errors of each and every digit in the fashion_mnist dataset\")\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can say from the above dataframe that label 6 has the least accuracy when compared to others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We have perfomed model tuning for different optimizers using 5-fold cross validation in the question 3. we have selected the optimizer with the best average accuracy(average of accuracies of all the 5 folds). so by using cross validation we can tune several model parameters in the same way like we did for selecting the best optimizer among ADAM and SGD.<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
